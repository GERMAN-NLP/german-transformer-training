[![Gitter](https://badges.gitter.im/German-Transformer-Training/community.svg)](https://gitter.im/German-Transformer-Training/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
[![GitHub license](https://img.shields.io/github/license/PhilipMay/german-transformer-training)](https://github.com/PhilipMay/german-transformer-training/blob/master/LICENSE)

# German Transformer Training
The goal of this repository is to plan the training of German transformer models.

## Links & Know-how Collection

NLP Libs
- [Fairseq](https://github.com/pytorch/fairseq)
- [Hugging Face](https://huggingface.co/) - [GitHub](https://github.com/huggingface)
- [FARM](https://farm.deepset.ai/) - [GitHub](https://github.com/deepset-ai/FARM)

Training
- Pre-training SmallBERTa - A tiny model to train on a tiny dataset: https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b
- Pretraining RoBERTa using your own data(Fairseq): https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md

## Datasets / Data Sources
- German Wikipedia
- Germeval 2017: https://sites.google.com/view/germeval2017-absa/data

## Contact
- [Gitter](https://gitter.im/German-Transformer-Training/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link)
